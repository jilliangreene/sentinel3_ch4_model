{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c36c88",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/jilliangreene/grad_school/sentinel3_ch4_model/venv/lib/python3.12/site-packages/gpboost/lib_gpboost.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <0755E224-5C72-303A-AA12-10FB17E0E6F6> /Users/jilliangreene/grad_school/sentinel3_ch4_model/venv/lib/python3.12/site-packages/gpboost/lib_gpboost.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpb\u001b[39;00m \u001b[38;5;66;03m#https://github.com/fabsig/GPBoost/tree/master; https://github.com/fabsig/GPBoost/blob/master/examples/python-guide/GPBoost_algorithm.py\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#%% Set seaborn theme and random state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grad_school/sentinel3_ch4_model/venv/lib/python3.12/site-packages/gpboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# coding: utf-8\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\" GPBoost\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m Original work Copyright (c) 2016 Microsoft Corporation. All rights reserved.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m Modified work Copyright (c) 2020 - 2024 Fabio Sigrist. All rights reserved.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, register_logger, GPModel, get_nested_categories\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (early_stopping, print_evaluation, record_evaluation,\n\u001b[32m      8\u001b[39m                        reset_parameter)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cv, train, CVBooster, grid_search_tune_parameters, tune_pars_TPE_algorithm_optuna\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grad_school/sentinel3_ch4_model/venv/lib/python3.12/site-packages/gpboost/basic.py:131\u001b[39m\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GPBoostError(lib.LGBM_GetLastError().decode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m NUMERIC_TYPES = (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_call\u001b[39m(ret):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/grad_school/sentinel3_ch4_model/venv/lib/python3.12/site-packages/gpboost/basic.py:122\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lib_path) == \u001b[32m0\u001b[39m:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m lib = \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdll\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m lib.LGBM_GetLastError.restype = ctypes.c_char_p\n\u001b[32m    124\u001b[39m callback = ctypes.CFUNCTYPE(\u001b[38;5;28;01mNone\u001b[39;00m, ctypes.c_char_p)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/ctypes/__init__.py:460\u001b[39m, in \u001b[36mLibraryLoader.LoadLibrary\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/ctypes/__init__.py:379\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: dlopen(/Users/jilliangreene/grad_school/sentinel3_ch4_model/venv/lib/python3.12/site-packages/gpboost/lib_gpboost.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <0755E224-5C72-303A-AA12-10FB17E0E6F6> /Users/jilliangreene/grad_school/sentinel3_ch4_model/venv/lib/python3.12/site-packages/gpboost/lib_gpboost.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "#%% IMPORT LIBRARIES\n",
    "\n",
    "#import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import gpboost as gpb #https://github.com/fabsig/GPBoost/tree/master; https://github.com/fabsig/GPBoost/blob/master/examples/python-guide/GPBoost_algorithm.py\n",
    "import time\n",
    "\n",
    "#%% Set seaborn theme and random state\n",
    "sns.set_theme(context='paper', style = 'ticks',font_scale=1.2)\n",
    "rs = 621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% READ DATA \n",
    "#read Landsat 8 and 9 data\n",
    "landsat = pd.read_csv(\"data/raw/JRC50/landsat_micorps_joined_jrc50_lagos.csv\")\n",
    "landsat = landsat.dropna(axis=0)\n",
    "\n",
    "#read sentinel-2 data\n",
    "s2 = pd.read_csv(\"data/raw/JRC50/sentinel_micorps_joined_jrc50_lagos.csv\")\n",
    "s2 = s2.dropna(axis=0)\n",
    "\n",
    "#shuffle the rows of the dataframes\n",
    "landsat = landsat.sample(frac=1).reset_index(drop=True)\n",
    "s2 = s2.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#%%rename Landsat band 5 to B5B8A and Sentinel-2 Band 8A to B5B8A\n",
    "#these are the NIR bands, which are named differently for Landsat and S2\n",
    "landsat = landsat.rename(columns={\"B5\": \"B5B8A\"})\n",
    "s2 = s2.rename(columns={\"B8A\": \"B5B8A\"})\n",
    "\n",
    "#%% Combine Landsat-S2 data for plotting/mapping\n",
    "landsat_s2 = pd.concat([landsat, s2], axis=0, join='inner')\n",
    "\n",
    "#%% CREATE NEW COLUMNS (Band ratios)\n",
    "\n",
    "from  itertools import combinations\n",
    "\n",
    "#Landsat\n",
    "landsat_bands = landsat.loc[:, ['B1', 'B2', 'B3', 'B4', 'B5B8A']]\n",
    "band_ratios_calc = {f'{a}/{b}': landsat_bands[a].div(landsat[b]) for a, b in combinations(landsat_bands.columns, 2)}\n",
    "landsat_band_ratios = pd.concat(band_ratios_calc, axis=1)\n",
    "landsat = pd.concat([landsat,landsat_band_ratios], axis=1)\n",
    "\n",
    "#Sentinel-2\n",
    "s2_bands = s2.loc[:, ['B1', 'B2', 'B3', 'B4', 'B5B8A']]\n",
    "band_ratios_s2_calc = {f'{a}/{b}': s2_bands[a].div(s2[b]) for a, b in combinations(s2_bands.columns, 2)}\n",
    "s2_band_ratios = pd.concat(band_ratios_s2_calc, axis=1)\n",
    "s2 = pd.concat([s2,s2_band_ratios], axis=1)\n",
    "\n",
    "#%%Convert dates to datetime\n",
    "#add column 't' to indicate days since start of sample data to use in Gaussian process model\n",
    "landsat['Date_Sampled'] = pd.to_datetime(landsat['Date_Sampled'], format = '%m/%d/%Y %H:%M:%S')\n",
    "landsat['image_date'] = pd.to_datetime(landsat['image_date'], format = '%m/%d/%Y %H:%M:%S')\n",
    "landsat['year'] = landsat['image_date'].dt.year\n",
    "landsat['month'] = landsat['image_date'].dt.month\n",
    "landsat['t'] = (landsat['image_date']-pd.to_datetime('04/30/2013 00:00:00', format = '%m/%d/%Y %H:%M:%S')).dt.days\n",
    "\n",
    "s2['Date_Sampled'] = pd.to_datetime(s2['Date_Sampled'], format = '%m/%d/%Y %H:%M:%S')\n",
    "s2['image_date'] = pd.to_datetime(s2['image_date'], format = '%m/%d/%Y %H:%M:%S')\n",
    "s2['year'] = s2['image_date'].dt.year\n",
    "s2['month'] = s2['image_date'].dt.month\n",
    "s2['t'] = (s2['image_date']-pd.to_datetime('04/30/2013 00:00:00', format = '%m/%d/%Y %H:%M:%S')).dt.days\n",
    "\n",
    "\n",
    "#%% Load LAGOS-NE IWS data, merged with dataframe\n",
    "iws_lulc = pd.read_csv(\"data/raw/lagos_ne/LAGOSNE_iws_lulc105.csv\")\n",
    "\n",
    "iws_lulc['iws_nlcd2011_pct_dev'] = iws_lulc['iws_nlcd2011_pct_21'] + \\\n",
    "    iws_lulc['iws_nlcd2011_pct_22'] + iws_lulc['iws_nlcd2011_pct_23'] + \\\n",
    "        iws_lulc['iws_nlcd2011_pct_24']\n",
    "        \n",
    "iws_lulc['iws_nlcd2011_pct_ag'] =  iws_lulc['iws_nlcd2011_pct_81'] +  iws_lulc['iws_nlcd2011_pct_82']\n",
    "\n",
    "iws_human = iws_lulc[['lagoslakeid','iws_nlcd2011_pct_dev', 'iws_nlcd2011_pct_ag']]\n",
    "\n",
    "#left_join landsat with iws_human\n",
    "\n",
    "landsat = landsat.merge(iws_human, on=\"lagoslakeid\")\n",
    "s2 = s2.merge(iws_human, on=\"lagoslakeid\")\n",
    "\n",
    "#%% Based on uneven lake-samples per HUC, combine based on geography\n",
    "\n",
    "huc4_group = {707: '0402-0403-0707', 402:'0402-0403-0707', \n",
    "              403: '0402-0403-0707', 405: '0405', 406: '0406', 407: '0407-0408', \n",
    "              408: '0407-0408', 409: '0409-0410', 410: '0409-0410'}\n",
    "\n",
    "landsat['HUC4_Group'] = landsat['HU4'].map(huc4_group)\n",
    "s2['HUC4_Group'] = s2['HU4'].map(huc4_group)\n",
    "\n",
    "#%% Get feature names\n",
    "\n",
    "#concatenate dataframes of original band values and ratios\n",
    "features = pd.concat([landsat_bands, landsat_band_ratios], axis=1, join=\"inner\")\n",
    "\n",
    "feature_names = list(features.columns)\n",
    "#feature_names.extend(['iws_nlcd2011_pct_dev','iws_nlcd2011_pct_ag'])\n",
    "\n",
    "#%% Find matching landsat and sentinel dates with secchi data. These matches will be used as a final test dataset\n",
    "#allowing comparision between performance of sentinel and Sentinel-2 models\n",
    "\n",
    "s2_landsat_match = landsat.merge(s2, right_on=['image_date', 'STORETID', 'Date_Sampled'], left_on=['image_date', 'STORETID','Date_Sampled'])\n",
    "#s2_landsat_match.to_csv('data/output/s2_landsat_secchi_match.csv')\n",
    "\n",
    "#%% create training and testing sets, where test set is coincident S2/Landsat data\n",
    "# Note that coincident data is from 2019-2022\n",
    "id_cols = s2_landsat_match[['image_date', 'STORETID', 'Date_Sampled']]\n",
    "\n",
    "#get test set based on matching image date, storetid, and sample date for secchi data\n",
    "landsat_test_set = landsat.merge(id_cols, how=\"inner\", on=['image_date', 'STORETID', 'Date_Sampled']).sort_values(['STORETID','Date_Sampled'])\n",
    "s2_test_set = s2.merge(id_cols, how=\"inner\", on=['image_date', 'STORETID', 'Date_Sampled']).sort_values(['STORETID','Date_Sampled'])\n",
    "\n",
    "#get training set based on data not in test_set\n",
    "landsat_train_set = landsat.merge(landsat_test_set.drop_duplicates(), how='left', indicator=True)\n",
    "landsat_train_set = landsat_train_set[landsat_train_set['_merge'] == 'left_only']\n",
    "\n",
    "s2_train_set = s2.merge(s2_test_set.drop_duplicates(), how='left', indicator=True)\n",
    "s2_train_set = s2_train_set[s2_train_set['_merge'] == 'left_only']\n",
    "\n",
    "# Cconcatenate training datasets, keep only common columns\n",
    "full_landsat_s2_train_set = pd.concat([landsat_train_set, s2_train_set], axis=0, join='inner')\n",
    "\n",
    "#%% Print training and testing data\n",
    "columns_to_print = ['STORETID','Lat','Lon','lagoslakeid','satellite','image_date',\n",
    "         'B1','B2', 'B3', 'B4', 'B5B8A',\n",
    "         'B1/B2','B1/B3','B1/B4',\n",
    "         'B1/B5B8A','B2/B3','B2/B4','B2/B5B8A',\n",
    "         'B3/B4','B3/B5B8A','B4/B5B8A',\n",
    "         'Date_Sampled', 'secchi_m','HU4','HU6','HU8','HU12','HUC4_Group',\n",
    "         'iws_nlcd2011_pct_dev',\n",
    "         'iws_nlcd2011_pct_ag',\n",
    "         't', 'month', 'year']\n",
    "\n",
    "full_landsat_s2_train_set.to_csv('data/processed/Combined_LandsatSentinel2_Secchi_trainingdata.csv',\n",
    "                         columns=columns_to_print, index=False)\n",
    "landsat_test_set.to_csv('data/processed/Landsat_Secchi_testdata.csv',\n",
    "                         columns=columns_to_print, index=False)\n",
    "s2_test_set.to_csv('data/processed/Sentinel2_Secchi_testdata.csv',\n",
    "                         columns=columns_to_print, index=False)\n",
    "\n",
    "#%% Define model evaluation function \n",
    "#Eval stats selected from Pontius \"Metrics that make a difference\" book.\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def evaluate(test_labels, predictions,  mod_name):\n",
    "    mean_dev = np.mean(predictions) - np.mean(test_labels)\n",
    "    mae = mean_absolute_error(test_labels, predictions)\n",
    "    pearson = stats.pearsonr(predictions, test_labels.ravel())[0]\n",
    "    slope, i = np.polyfit(test_labels,predictions,1)\n",
    "    r2 = r2_score(test_labels, predictions)\n",
    "    rmse = mean_squared_error(test_labels, predictions, squared=False)\n",
    "    print('Model Performance')\n",
    "    print('Mean Deviation: {:0.3f} m'.format(np.mean(mean_dev)))\n",
    "    print('Mean Absolute Error: {:0.3f} m'.format(mae))\n",
    "    print('RMSE: {:0.3f} m'.format(rmse))\n",
    "    print('Slope = {:0.3f}'.format(slope))\n",
    "    print('Pearson correlation = {:0.3f}'.format(pearson))\n",
    "    print('r2 = {:0.3f}'.format(r2))\n",
    "    values = [mean_dev, mae, rmse, slope, pearson, r2]\n",
    "    metrics = pd.DataFrame(values, \n",
    "                           columns = ['statistic'])\n",
    "    metrics.index = ['MeanDeviation','MAE','RMSE', 'Slope',\n",
    "               'Pearson', 'r2']\n",
    "    metrics.to_csv('data/results/model_statistics_' + mod_name +'.csv', index=True)\n",
    "\n",
    "\n",
    "#%% Function \"prep_data\"\n",
    "#Prep data for use in the models - give the data the original dataframe\n",
    "\n",
    "def prep_data(training_df, testing_df):\n",
    "    x_train = training_df[feature_names].to_numpy()\n",
    "    x_test = testing_df[feature_names].to_numpy()\n",
    "    \n",
    "    #response training and testing\n",
    "    y_train = training_df['secchi_m'].to_numpy()\n",
    "    y_test = testing_df['secchi_m'].to_numpy()\n",
    "    \n",
    "    lake = training_df['lagoslakeid'].to_numpy()\n",
    "    lake_test = testing_df['lagoslakeid'].to_numpy()\n",
    "    \n",
    "    huc = training_df['HU4'].to_numpy()\n",
    "    huc_test = testing_df['HU4'].to_numpy()\n",
    "    \n",
    "    huc4_grp = training_df['HUC4_Group'].to_numpy()\n",
    "    huc4_grp_test = testing_df['HUC4_Group'].to_numpy()\n",
    "    \n",
    "    year = training_df['year'].to_numpy()\n",
    "    year_test = testing_df['year'].to_numpy()\n",
    "    \n",
    "    month = training_df['month'].to_numpy()\n",
    "    month_test = testing_df['month'].to_numpy()\n",
    "    \n",
    "    t_train = training_df['t'].to_numpy()\n",
    "    t_test = testing_df['t'].to_numpy()\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, \\\n",
    "        lake, lake_test, huc, huc_test, huc4_grp, huc4_grp_test, \\\n",
    "            year, year_test, month, month_test, t_train, t_test\n",
    "\n",
    "#%% GPBOOST - Grid search tuning and cv boost function\n",
    "\n",
    "# arguments = split type, experiment name (e.g. Landsat, year splits)\n",
    "# returns a dictionary of best parameters and model performance metrics\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.5, 1], \n",
    "              'min_data_in_leaf': [5,10,100,500],\n",
    "              'max_depth': [-1, 1, 2, 3, 5, 10],\n",
    "              'lambda_l2': [0,1,10, 100]}\n",
    "\n",
    "    #simple parameter grid for testing\n",
    "    # param_grid = {'learning_rate': [0.5], \n",
    "    #               'min_data_in_leaf': [10],\n",
    "    #               'max_depth': [1,2],\n",
    "    #               'lambda_l2': [0,1]}\n",
    "\n",
    "def model_tune_crossval(splits, par_grid, experiment_name):\n",
    "    \n",
    "    other_params = {'num_leaves': 2**10, 'verbose': 0}\n",
    "    \n",
    "    #set up panel model with random effects = lake\n",
    "    #temporal random slopes = days since 4/30/2013 (start of dataset)\n",
    "    gp_model = gpb.GPModel(group_data=pd.DataFrame({'lake':lake}), \n",
    "                           likelihood=\"gaussian\",\n",
    "                           group_rand_coef_data=t_train, \n",
    "                           ind_effect_group_rand_coef=[1])\n",
    "\n",
    "    data_train = gpb.Dataset(data=x_train, label=y_train)\n",
    "\n",
    "    #initialize grid search (note mae will be listed as 'l1-mean' in output)\n",
    "    opt_pars = gpb.grid_search_tune_parameters(param_grid=par_grid, \n",
    "                                                 params=other_params,\n",
    "                                                 num_boost_round=1000,\n",
    "                                                 num_try_random=None, \n",
    "                                                 folds = splits, \n",
    "                                                 seed=81388,\n",
    "                                                 train_set=data_train, \n",
    "                                                 gp_model=gp_model,\n",
    "                                                 use_gp_model_for_validation=True, \n",
    "                                                 verbose_eval=0,\n",
    "                                                 early_stopping_rounds=10,\n",
    "                                                 metric=[\"mae\",\"rmse\"])                                             \n",
    "    \n",
    "    # Get best number of boosting iterations\n",
    "    cvbst = gpb.cv(params=opt_pars['best_params'], \n",
    "                   train_set=data_train,\n",
    "                   gp_model=gp_model, \n",
    "                   use_gp_model_for_validation=True,\n",
    "                   num_boost_round=1000, \n",
    "                   early_stopping_rounds=10,\n",
    "                   folds = splits, \n",
    "                   show_stdv=False, \n",
    "                   seed=22587, \n",
    "                   metric=[\"mae\",\"rmse\"])\n",
    "    \n",
    "    best_iter_index = np.argmin(cvbst['l1-mean']) #cv boost best iteration\n",
    "    \n",
    "    best_pars = opt_pars['best_params']\n",
    "    \n",
    "    #add model results to dictionary\n",
    "    best_pars['iterations'] = best_iter_index +1\n",
    "    best_pars['mae-mean'] = cvbst['l1-mean'][best_iter_index]\n",
    "    best_pars['mae-stdv'] = cvbst['l1-stdv'][best_iter_index]\n",
    "    best_pars['rmse-mean'] = cvbst['rmse-mean'][best_iter_index]\n",
    "    best_pars['rmse-stdv'] = cvbst['rmse-stdv'][best_iter_index]\n",
    "    best_pars['experiment'] =  experiment_name\n",
    "    \n",
    "    return(best_pars)\n",
    "    \n",
    "\n",
    "#%% LANDSAT: split data by holdout dataset of coincident Landsat-S2\n",
    "#must call this function prior to calling the cross val function to train model\n",
    "x_train, x_test, y_train, y_test, \\\n",
    "    lake, lake_test, huc, huc_test, \\\n",
    "        huc4_grp, huc4_grp_test, \\\n",
    "            year, year_test, month, month_test, \\\n",
    "                t_train, t_test = prep_data(landsat_train_set, landsat_test_set)\n",
    "\n",
    "#%% LANDSAT- YEAR SPLITS, GRID SEARCH TUNE PARAMETERS\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "#split by year\n",
    "logo = LeaveOneGroupOut()\n",
    "splits_year = list(logo.split(x_train, y_train, groups=year))\n",
    "\n",
    "#dictionary of best simulation results is returned (parameters and error stats)\n",
    "results_yr_fold = model_tune_crossval(splits_year, param_grid, \"Landsat-Year-Folds\")\n",
    "\n",
    "#%% LANDSAT- HUC4 Group SPLITS, GRID SEARCH TUNE PARAMETERS\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "splits_huc = list(logo.split(x_train, y_train, groups=huc4_grp))\n",
    "\n",
    "#dictionary of best simulation results is returned (parameters and error stats)\n",
    "results_huc_fold = model_tune_crossval(splits_huc, param_grid, \"Landsat-HUC-Folds\")\n",
    "\n",
    "#%% LANDSAT- LAKE (lagoslakeid) SPLITS, w/ 10-fold, GRID SEARCH TUNE PARAMETERS\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=10)\n",
    "splits_lake = list(group_kfold.split(x_train, y_train, groups=lake))\n",
    "\n",
    "#dictionary of best simulation results is returned (parameters and error stats)\n",
    "results_lake_10fold = model_tune_crossval(splits_lake, param_grid, \"Landsat-Lake-Folds\")\n",
    "\n",
    "#%% LANDSAT: SIMPLE 10-FOLD CROSS VALIDATION (NO GROUPS)\n",
    "from sklearn.model_selection import KFold\n",
    "         \n",
    "kfold = KFold(n_splits = 10, random_state = 100, shuffle = True)\n",
    "\n",
    "#dictionary of best simulation results is returned (parameters and error stats)\n",
    "results_10fold = model_tune_crossval(kfold, param_grid, \"Landsat-10-Folds\")\n",
    "\n",
    "#START RUN HERE\n",
    "\n",
    "#%% LANDSAT: GRADIENT TREE-BOOSTING with NO GROUP VARIABLE (NO RANDOM EFFECTS)\n",
    "\n",
    "\n",
    "# param_grid = {'learning_rate': [0.5, 0.1], \n",
    "#               'min_data_in_leaf': [10,100],\n",
    "#               'max_depth': [1,2],\n",
    "#               'lambda_l2': [0,1]}\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.5, 1], \n",
    "              'min_data_in_leaf': [5,10,100,500],\n",
    "              'max_depth': [-1, 1, 2, 3, 5, 10],\n",
    "              'lambda_l2': [0,1,10, 100]}\n",
    "other_params = {'num_leaves': 2**10, 'verbose': 0}\n",
    "    \n",
    "kfold = KFold(n_splits = 10, random_state = 100, shuffle = True)\n",
    "\n",
    "data_train = gpb.Dataset(data=x_train, label=y_train)\n",
    "\n",
    "#initialize grid search (note mae will be listed as 'l1-mean' in output)\n",
    "opt_pars_gbm = gpb.grid_search_tune_parameters(gp_model=None,\n",
    "                                           param_grid=param_grid,\n",
    "                                           params=other_params,\n",
    "                                           num_boost_round=1000,\n",
    "                                           num_try_random=None, \n",
    "                                           folds = kfold, \n",
    "                                           seed=81388,\n",
    "                                           train_set=data_train, \n",
    "                                           use_gp_model_for_validation=False, \n",
    "                                           verbose_eval=0,\n",
    "                                           early_stopping_rounds=10,\n",
    "                                           metric=[\"mae\", \"rmse\"])                                             \n",
    "                                               \n",
    "    # Get best number of boosting iterations\n",
    "cvbst = gpb.cv(params=opt_pars_gbm['best_params'], \n",
    "               train_set=data_train,\n",
    "               num_boost_round=1000, \n",
    "               early_stopping_rounds=10,\n",
    "               folds = kfold, \n",
    "               show_stdv=False, \n",
    "               seed=22587, \n",
    "               metric=[\"mae\", \"rmse\"])\n",
    "\n",
    "best_iter_index_gbm = np.argmin(cvbst['l1-mean']) #cv boost best iteration\n",
    "\n",
    "best_pars_gbm = opt_pars_gbm['best_params']\n",
    "\n",
    "#add model results to dictionary\n",
    "best_pars_gbm['iterations'] = best_iter_index_gbm +1\n",
    "best_pars_gbm['mae-mean'] = cvbst['l1-mean'][best_iter_index_gbm]\n",
    "best_pars_gbm['mae-stdv'] = cvbst['l1-stdv'][best_iter_index_gbm]\n",
    "best_pars_gbm['rmse-mean'] = cvbst['rmse-mean'][best_iter_index_gbm]\n",
    "best_pars_gbm['rmse-stdv'] = cvbst['rmse-stdv'][best_iter_index_gbm]\n",
    "best_pars_gbm['experiment'] =  \"Landsat-GBM-FixedEffects-10fold\"\n",
    "\n",
    "#%% LANDSAT GPB: COMBINE cross validation dictionary results, write to csv\n",
    "import collections\n",
    "\n",
    "dict_list = [results_yr_fold, results_huc_fold,\n",
    "                         results_lake_10fold, results_10fold,\n",
    "                         best_pars_gbm]\n",
    "\n",
    "landsat_results_gridsearch = collections.defaultdict(list)\n",
    "for d in dict_list:\n",
    "    for k, v in d.items():  # d.items() in Python 3+\n",
    "        landsat_results_gridsearch[k].append(v)\n",
    "\n",
    "pd.DataFrame.from_dict(data=landsat_results_gridsearch, orient='index') \\\n",
    "    .to_csv('data/results/landsat_gridsearch_results.csv', header=False)\n",
    "    \n",
    "\n",
    "#%% LANDSAT ONE MORE TUNE (10-fold) to refine\n",
    "\n",
    "param_grid_2 = {'learning_rate': [0.005, 0.01, 0.02], \n",
    "              'min_data_in_leaf': [3,5,7],\n",
    "              'max_depth': [-1, 10, 15],\n",
    "              'lambda_l2': [80,100,120]} \n",
    "\n",
    "kfold = KFold(n_splits = 10, random_state = 100, shuffle = True)\n",
    "\n",
    "#dictionary of best simulation results is returned (parameters and error stats)\n",
    "results_10fold_finaltune = model_tune_crossval(kfold, param_grid_2, \"Landsat-10-Folds-FinalTune\")\n",
    "\n",
    "\n",
    "#%% LANDSAT GPB FINAL MODEL TUNE NUM ROUNDS\n",
    "\n",
    "#best parameters are from best pars of regular k-fold, final tuning\n",
    "best_pars = {'lambda_l2': 80, \n",
    "                  'learning_rate': 0.02, \n",
    "                  'max_depth': -1, \n",
    "                  'min_data_in_leaf': 5}\n",
    "\n",
    "data_train = gpb.Dataset(data=x_train, label=y_train)\n",
    "\n",
    "gp_model = gpb.GPModel(group_data=pd.DataFrame({'lake':lake}), \n",
    "                       likelihood=\"gaussian\",\n",
    "                       group_rand_coef_data=t_train, \n",
    "                       ind_effect_group_rand_coef=[1])\n",
    "#redo boost rounds to get best number\n",
    "\n",
    "cvbst = gpb.cv(params=best_pars, \n",
    "               train_set=data_train,\n",
    "               gp_model=gp_model, \n",
    "               use_gp_model_for_validation=True,\n",
    "               num_boost_round=3000, \n",
    "               early_stopping_rounds=10,\n",
    "               folds = kfold, \n",
    "               show_stdv=False, \n",
    "               seed=22587, \n",
    "               metric=[\"mae\",\"rmse\"])\n",
    "\n",
    "best_iter_index_gpb = np.argmin(cvbst['l1-mean']) #cv boost best iteration\n",
    "\n",
    "#%%\n",
    "gpb_landsat = gpb.train(params=best_pars, \n",
    "                        train_set=data_train,  \n",
    "                        gp_model=gp_model,\n",
    "                        num_boost_round=best_iter_index_gpb)\n",
    "#gp_model.summary() # Estimated random effects model\n",
    "\n",
    "# Save model\n",
    "gpb_landsat.save_model('data/output/model_gpb_landsat.json')\n",
    "# Load from file and make predictions again\n",
    "#bst_loaded = gpb.Booster(model_file = 'model.json')\n",
    "\n",
    "#other way to save the model with joblib\n",
    "import joblib\n",
    "joblib.dump(gpb_landsat, 'data/output/model_gpb_landsat.pkl')\n",
    "# load model\n",
    "#bst_loaded = joblib.load('data/output/model_gpb_landsat.pkl')\n",
    "\n",
    "#%% LANDSAT GPB TRAINED MODEL PREDICTIONS ON HOLDOUT DATASET\n",
    "# Predict response variable (pred_latent=False)\n",
    "# pred_resp['response_mean']: mean predictions of the response variable \n",
    "# which combines predictions from the tree ensemble and the random effects\n",
    "# pred_resp['response_var']: predictive (co-)variances (if predict_var=True)\n",
    "pred_resp = gpb_landsat.predict(data=x_test, \n",
    "                                        group_data_pred=pd.DataFrame({'lake':lake_test}),\n",
    "                                        group_rand_coef_data_pred = t_test,\n",
    "                                        predict_var=False,\n",
    "                                        pred_latent=False)\n",
    "\n",
    "#extract predictions\n",
    "preds_landsat_testset = pred_resp['response_mean']\n",
    "\n",
    "#%% Plot prediction on test dataset (Landsat model)\n",
    "#Test dataset = coincident Landsat+Sentinel data\n",
    "#Using Landsat reflectance values\n",
    "#https://stackoverflow.com/questions/19064772/visualization-of-scatter-plots-with-overlapping-points-in-matplotlib\n",
    "\n",
    "values = np.vstack([y_test.ravel(), preds_landsat_testset.ravel()])\n",
    "kernel = stats.gaussian_kde(values, bw_method= 0.5)(values)\n",
    "\n",
    "plt.scatter(y_test, preds_landsat_testset, s=20, c=kernel,cmap='viridis')\n",
    "plt.axline((0,0), (13,13), linewidth=1, color='black')\n",
    "plt.axis((0,13,0,13))\n",
    "plt.xlabel(\"In-situ SDD (m)\")\n",
    "plt.ylabel(\"Predicted SDD (m)\")\n",
    "plt.colorbar()\n",
    "plt.savefig('data/figures/gpboost/GPBoost_Landsat_coincident_data_holdout'+ time.strftime(\"%Y%m%d-%H%M%S\")+ '.jpg', dpi=300)\n",
    "\n",
    "evaluate(y_test, preds_landsat_testset, 'GPB_Landsat_testset')\n",
    "\n",
    "#%% GPB Print final predictions on test set\n",
    "\n",
    "landsat_test_set[\"secchi_pred_m\"] = preds_landsat_testset\n",
    "\n",
    "landsat_preds_out = landsat_test_set[[\"lagoslakeid\", \"STORETID\", \"Lat\", \"Lon\",\n",
    "                                     \"satellite\", \"pathrow\", \"image_date\", \"Date_Sampled\",\n",
    "                                     \"secchi_m\", \"secchi_pred_m\"]]\n",
    "\n",
    "landsat_preds_out.to_csv(\"data/results/GPB_landsat_testset_predictions.csv\", index=False)    \n",
    "\n",
    "\n",
    "#%% LANDSAT GBM (NO RANDOM EFFECTS): TRAIN BEST MODEL\n",
    "\n",
    "gbm = gpb.train(params=best_pars_gbm, \n",
    "                        train_set=data_train,  \n",
    "                        num_boost_round=best_iter_index_gbm)\n",
    "\n",
    "pred_resp_gbm = gbm.predict(data=x_test, predict_var=False, pred_latent=False)\n",
    "\n",
    "evaluate_gbm = evaluate(y_test, pred_resp_gbm, 'GBM_Landsat')\n",
    "\n",
    "values = np.vstack([y_test.ravel(), pred_resp_gbm.ravel()])\n",
    "kernel = stats.gaussian_kde(values, bw_method= 0.5)(values)\n",
    "\n",
    "plt.scatter(y_test, pred_resp_gbm, s=20, c=kernel, cmap='viridis')\n",
    "plt.axline((0,0), (13,13), linewidth=1, color='black')\n",
    "plt.axis((0,13,0,13))\n",
    "plt.xlabel(\"In-situ SDD (m)\")\n",
    "plt.ylabel(\"Predicted SDD (m)\")\n",
    "plt.colorbar()\n",
    "plt.savefig('data/figures/gbm/GBM_Landsat_Coincident_holdout.jpg', dpi=300)\n",
    "\n",
    "#%% GBM (NO RANDOM EFFECTS) Print final predictions\n",
    "\n",
    "landsat_test_set[\"secchi_pred_m\"] = pred_resp_gbm\n",
    "\n",
    "landsat_preds_out_gbm = landsat_test_set[[\"lagoslakeid\", \"STORETID\", \"Lat\", \"Lon\",\n",
    "                                     \"satellite\", \"pathrow\", \"image_date\", \"Date_Sampled\",\n",
    "                                     \"secchi_m\", \"secchi_pred_m\"]]\n",
    "\n",
    "landsat_preds_out_gbm.to_csv(\"data/results/GBM_landsat_testset_predictions.csv\", index=False)\n",
    "\n",
    "#%% LANDSAT GPB: Plot SHAP \n",
    "#https://github.com/AidanCooper/shap-analysis-guide/blob/main/analysis.ipynb\n",
    "import shap\n",
    "\n",
    "Xpd = landsat_train_set[feature_names]\n",
    "\n",
    "shap_values = shap.Explainer(gpb_landsat).shap_values(Xpd)\n",
    "shap.summary_plot(shap_values, Xpd, show=False)\n",
    "plt.savefig('data/figures/gpboost/shap_landsat.jpg', dpi=300)\n",
    "\n",
    "#%%TRAIN FINAL LANDSAT MODEL AND APPLY TO ALL REFLECTANCE DATA\n",
    "\n",
    "best_pars = {'lambda_l2': 80, \n",
    "                  'learning_rate': 0.02, \n",
    "                  'max_depth': -1, \n",
    "                  'min_data_in_leaf': 5}\n",
    "\n",
    "groups = landsat[\"lagoslakeid\"].to_numpy()\n",
    "time = landsat[\"t\"].to_numpy()\n",
    "\n",
    "gpb_model_landsat_final = gpb.GPModel(group_data=pd.DataFrame({'lake': groups}), \n",
    "                       likelihood=\"gaussian\",\n",
    "                       group_rand_coef_data= time, \n",
    "                       ind_effect_group_rand_coef=[1])\n",
    "\n",
    "data_train_final = gpb.Dataset(data=landsat[feature_names].to_numpy(), \n",
    "                         label=landsat[\"secchi_m\"].to_numpy())\n",
    "\n",
    "gpb_landsat_final_trained = gpb.train(params=best_pars, \n",
    "                        train_set=data_train_final,  \n",
    "                        gp_model=gpb_model_landsat_final,\n",
    "                        num_boost_round=1146)\n",
    "\n",
    "gpb_model_landsat_final.summary() # Estimated random effects model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
